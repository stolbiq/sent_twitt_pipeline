{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "--- Tweeter parser ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = 'r8BGSocOtv6vZBnjcwkU5qM4u'\n",
    "consumer_secret = 'CNAJg23IWXLvUUyJfuFUQOYfzvtKfQVQu6SsY82yJY7GGdhBDC'\n",
    "access_token = '796083347315367936-grZPgV2cW8DvrozevvihPIJ1Oq9WQVK'\n",
    "access_secret = 'oJoGFZYDDOXjyG30v0FBChWmGfF9RG6fXvDCVuNrDZJGb'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def process_or_store(tweet):\n",
    "#    print(json.dumps(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import json\n",
    "#for tweet in tweepy.Cursor(api.user_timeline).items():\n",
    "#    process_or_store(tweet._json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    " \n",
    "class MyListener(StreamListener):\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('negative.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print('&quot; Error on_data: %s&quot; % str(e)')\n",
    "            return True\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "    \n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "\n",
    "#--- filter for positive tweets \n",
    "#twitter_stream.filter(track=[':)', ':-)', '=)', ':D', ';-)', ';)', '(-;', '(-:', '(:', '(;', '(=', ':d', ':p'], languages=['fr'])\n",
    "\n",
    "#--- filter for negative tweets\n",
    "twitter_stream.filter(track=[':(', ':-(', '=(', ')-:', '):', ');', ')=', \":'(\", \":'‑(\"], languages=['fr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    " \n",
    "class MyListener(StreamListener):\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('positive.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print('&quot; Error on_data: %s&quot; % str(e)')\n",
    "            return True\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "    \n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "\n",
    "#--- filter for positive tweets \n",
    "twitter_stream.filter(track=[':)', ':-)', '=)', ':D', ';-)', ';)', '(-;', '(-:', '(:', '(;', '(=', ':d', ':p'], languages=['fr'])\n",
    "\n",
    "#--- filter for negative tweets\n",
    "#twitter_stream.filter(track=[':(', ':-(', '=(', ')-:', '):', ');', ')=', \":'(\", \":'‑(\"], languages=['fr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Preprocessing part ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "smiles_negativ = [':(', ':-(', '=(', ')-:', '):', ');', ')=']\n",
    "smiles_positiv = [':)', ':-)', '=)', ':D', ';-)', ';)', '(-;', '(-:', '(:', '(;', '(=', ':d', ':p']\n",
    "pronoms = ['je', 'tu', 'nous', 'vous', 'il', 'elle', 'ils', 'elles', 'ça', '️']\n",
    "smiles = smiles_negativ + smiles_positiv\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('french') + punctuation + smiles + ['rt', 'les', '…', 'é', 'ca', 'ée', 'cette', '’', 'a', 'leurs', ''] + pronoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str1 = [\n",
    "    r\"(?:[\\w_]+\\s+pas)\", # concat \"pas\" with the preceding word\n",
    "    r\"(?:pas+\\s+[\\w_]+[\\w_]+[\\w_]+)\",\n",
    "    r\"(?:[a-z][a-z\\-_]+[a-z])\", # words with - \n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "\n",
    "regex_str2 = [\n",
    "    r\"(?:pas+\\s+[\\w_]+[\\w_]+[\\w_]+)\",\n",
    "    r'(?<=pas\\s)(?<=\\s)*très\\s+[\\w_]+', # concat \"très\" + word if there is no \"pas\" behind\n",
    "    r'(?<=pas\\s)(?<=\\s)*trop\\s+[\\w_]+', # \"trop\" + word if there is no \"pas\" behind] # concat \"pas\" with the following word\n",
    "    r'(?<=pas\\s)(?<=\\s)*moins\\s+[\\w_]+'\n",
    "    ]\n",
    "\n",
    "regex_str3 = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "]\n",
    "\n",
    "regex_str4 = [\n",
    "    r'(?<=pas\\s)(?<=\\s)*[\\w_]+',# looks for the word that follows \"pas \" (to eliminate it afterwards)\n",
    "    r'(?<=trop\\s)(?<=\\s)*[\\w_]+',\n",
    "    r'(?<=très\\s)(?<=\\s)*[\\w_]+',\n",
    "    r'(?<=moins\\s)(?<=\\s)*[\\w_]+',\n",
    "    ] \n",
    "\n",
    "tokens_re1 = re.compile(r'('+'|'.join(regex_str1)+')', re.VERBOSE | re.IGNORECASE)\n",
    "tokens_re2 = re.compile(r'('+'|'.join(regex_str2)+')', re.VERBOSE | re.IGNORECASE)\n",
    "tokens_re3 = re.compile(r'('+'|'.join(regex_str3)+')', re.VERBOSE | re.IGNORECASE)\n",
    "tokens_re4 = re.compile(r'('+'|'.join(regex_str4)+')', re.VERBOSE | re.IGNORECASE)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = FrenchStemmer()\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def preprocess(s):\n",
    "    ### HTML tags, @-mentions, # hash-tags, # URLs, # numbers elimination\n",
    "    s2 = re.sub(tokens_re3, \"\",s)\n",
    "    ### simple tokinization (default function)\n",
    "    tokens_init = word_tokenize(s2)\n",
    "    ### stemming\n",
    "    tokens_init = stem_tokens(tokens_init)\n",
    "    ### string recreation\n",
    "    s2 = \"\".join([\" \"+i for i in tokens_init]).strip()\n",
    "    ### finding all non matching combinations from tokens_re1 in s2\n",
    "    tokens1 = tokens_re1.findall(s2)\n",
    "    ### ...from tokens_re4 in s2\n",
    "    tokens4 = tokens_re4.findall(s2)\n",
    "    ### cleaning tokens1 from stop-words and tokens4\n",
    "    tokens1 = [term for term in tokens1 if term not in stop and term not in tokens4]\n",
    "    ### \"pas\" + word\n",
    "    tokens2 = tokens_re2.findall(s2)\n",
    "    \n",
    "    tokens = tokens1 + tokens2\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['est pas', 'aim pas', 'pas trop', 'veux pas', 'aller', 'pas ici', 'pas trop']"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"RT C'est pas ici @hoho: ne t'aime pas ! pas trop intelligente je ne veux pas y aller :D http://hoho.com #NLP\"\n",
    "preprocess(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Tweets' extracting from the json file ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "\n",
    "tweets_text = []\n",
    "mood = []\n",
    "count_all = Counter()\n",
    "\n",
    "for line in open('positive.json'):\n",
    "  try: \n",
    "    new_tweet = json.loads(line)\n",
    "    text = new_tweet['text']\n",
    "    tweets_text.append(text)\n",
    "    mood.append(1)\n",
    "    count_all.update(preprocess(text))\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "\n",
    "for line in open('negative.json'):\n",
    "  try: \n",
    "    new_tweet = json.loads(line)\n",
    "    text = new_tweet['text']\n",
    "    tweets_text.append(text)\n",
    "    mood.append(0)\n",
    "    count_all.update(preprocess(text))\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#count_all.most_common(2000)\n",
    "less_frequent = count_all\n",
    "for word in list(less_frequent):\n",
    "    if less_frequent[word] > 5:\n",
    "        del less_frequent[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Tf-Idf matrix ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stops = less_frequent.keys()\n",
    "tfidf = TfidfVectorizer(tokenizer=preprocess, stop_words = stops)\n",
    "A = tfidf.fit_transform(tweets_text)\n",
    "A = A.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8544, 1470)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#U, S, V = np.linalg.svd(A)\n",
    "#print (U.shape)\n",
    "#print (S.shape)\n",
    "#print (V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#N = 500\n",
    "#M = dot(dot(U[:,:N],diag(S)[:N,:N]),V[:N,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8544, 1470)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#V2 = V[:500,:]\n",
    "#M = np.dot(A,V2.transpose())\n",
    "M = A\n",
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Algorithmic testing part ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as rf\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X = M\n",
    "Y = mood\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = rf()\n",
    "model.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)\n",
    "pred = np.round(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24485018726591759"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.abs(y_test - pred))/len(y_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
