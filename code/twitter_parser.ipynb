{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "--- Tweeter parser ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = 'r8BGSocOtv6vZBnjcwkU5qM4u'\n",
    "consumer_secret = 'CNAJg23IWXLvUUyJfuFUQOYfzvtKfQVQu6SsY82yJY7GGdhBDC'\n",
    "access_token = '796083347315367936-grZPgV2cW8DvrozevvihPIJ1Oq9WQVK'\n",
    "access_secret = 'oJoGFZYDDOXjyG30v0FBChWmGfF9RG6fXvDCVuNrDZJGb'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def process_or_store(tweet):\n",
    "#    print(json.dumps(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import json\n",
    "#for tweet in tweepy.Cursor(api.user_timeline).items():\n",
    "#    process_or_store(tweet._json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    " \n",
    "class MyListener(StreamListener):\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('negative.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print('&quot; Error on_data: %s&quot; % str(e)')\n",
    "            return True\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "    \n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "\n",
    "#--- filter for positive tweets \n",
    "#twitter_stream.filter(track=[':)', ':-)', '=)', ':D', ';-)', ';)', '(-;', '(-:', '(:', '(;', '(=', ':d', ':p'], languages=['fr'])\n",
    "\n",
    "#--- filter for negative tweets\n",
    "twitter_stream.filter(track=[':(', ':-(', '=(', ')-:', '):', ');', ')='], languages=['fr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Preprocessing part ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "smiles_negativ = [':(', ':-(', '=(', ')-:', '):', ');', ')=']\n",
    "smiles_positiv = [':)', ':-)', '=)', ':D', ';-)', ';)', '(-;', '(-:', '(:', '(;', '(=', ':d', ':p']\n",
    "pronoms = ['je', 'tu', 'nous', 'vous', 'il', 'elle', 'ils', 'elles', 'ça', 'jsuis']\n",
    "smiles = smiles_negativ + smiles_positiv\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('french') + punctuation + smiles + ['rt', 'les', '…', 'é', 'ca', 'ée', 'cette', '’', 'a', 'leurs', ''] + pronoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:pas+\\s+[\\w_]+[\\w_]+[\\w_]+)\", # concat pas with the following word\n",
    "    r\"(?:[\\w_]+\\s+pas)\", # concat pas with the preceding word\n",
    "    r\"(?:très\\s+[\\w_]+)\", # concat très + word\n",
    "    r\"(?:trop\\s+[\\w_]+)\", # trop + word\n",
    "    r\"(?:n'[\\w_]+)\", # n' + word\n",
    "    #r\"(?:ne\\s+[\\w_][^'][\\w_]*)\",\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z\\-_]+[a-z])\", # words with - \n",
    "    #r\"(?:[^jtm]+'[a-z]+)\", # words with '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = FrenchStemmer()\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def preprocess(s):\n",
    "    tokens = tokens_re.findall(s)\n",
    "    tokens = [term.lower() for term in tokens if term.lower() \n",
    "                not in stop \n",
    "                and not term.isdigit() \n",
    "                and not term.startswith(('@', '#', 'http'))]\n",
    "    #tokens = stem_tokens(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tweet = \"RT C'est pas ici @hoho: je ne t'aime pas ! trop intelligent je ne veux pas y aller :D http://hoho.com #NLP\"\n",
    "#preprocess(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Tweets' cleaning and tokenizing ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "\n",
    "tweets_text = []\n",
    "mood = []\n",
    "count_all = Counter()\n",
    "\n",
    "for line in open('positive.json'):\n",
    "  try: \n",
    "    new_tweet = json.loads(line)\n",
    "    text = new_tweet['text']\n",
    "    tweets_text.append(text)\n",
    "    mood.append(1)\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "\n",
    "for line in open('negative.json'):\n",
    "  try: \n",
    "    new_tweet = json.loads(line)\n",
    "    text = new_tweet['text']\n",
    "    tweets_text.append(text)\n",
    "    mood.append(0)\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Tf-Idf matrix ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(tokenizer=preprocess)\n",
    "A = tfidf.fit_transform(tweets_text)\n",
    "A = A.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.09268701,  0.11435201, ...,  0.95361125,\n",
       "        0.97844342,  1.        ])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4917, 4917)\n",
      "(4917,)\n",
      "(7324, 7324)\n"
     ]
    }
   ],
   "source": [
    "U, S, V = np.linalg.svd(A)\n",
    "print (U.shape)\n",
    "print (S.shape)\n",
    "print (V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4917, 500)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V2 = V[:500,:]\n",
    "M = np.dot(A,V2.transpose())\n",
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Algorithmic testing part ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as rf\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X = M\n",
    "Y = mood\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = rf()\n",
    "model.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)\n",
    "pred = np.round(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19099717659137594"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "r2_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for var_text in tweets_text:\n",
    "#    tweet_prep = preprocess(var_text)\n",
    "#    print(tweet_prep)\n",
    "    #f.write(joined_terms)\n",
    "    #terms_bigram = bigrams(terms_all)\n",
    "    #print(terms_all)\n",
    "    #count_all.update(terms_all)\n",
    "#f.close()\n",
    "#print(count_all.most_common(100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
